{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized String Operations\n",
    "\n",
    "* There is a Pandas way of doing this that is much more terse and compact\n",
    "* Pandas has a set of String operations that do much painful work for you\n",
    "* Especially handling bad data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['peter', 'Paul', 'MARY', 'gUIDO']\n",
    "\n",
    "for s in data:\n",
    "    print(s.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But like above, this breaks very easily with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\n",
    "\n",
    "for s in data:\n",
    "    print(s.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Pandas library has *vectorized string operations* that handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = pd.Series(data)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names.str.capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look ma! No errors!\n",
    "* Pandas includes a a bunch of methods for doing things to strings.\n",
    "\n",
    "|             |                  |                  |                  |\n",
    "|-------------|------------------|------------------|------------------|\n",
    "|``len()``    | ``lower()``      | ``translate()``  | ``islower()``    | \n",
    "|``ljust()``  | ``upper()``      | ``startswith()`` | ``isupper()``    | \n",
    "|``rjust()``  | ``find()``       | ``endswith()``   | ``isnumeric()``  | \n",
    "|``center()`` | ``rfind()``      | ``isalnum()``    | ``isdecimal()``  | \n",
    "|``zfill()``  | ``index()``      | ``isalpha()``    | ``split()``      | \n",
    "|``strip()``  | ``rindex()``     | ``isdigit()``    | ``rsplit()``     | \n",
    "|``rstrip()`` | ``capitalize()`` | ``isspace()``    | ``partition()``  | \n",
    "|``lstrip()`` |  ``swapcase()``  |  ``istitle()``   | ``rpartition()`` |\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "* In the cells below, try three of the string operations listed above on the Pandas Series `monte`\n",
    "* Remember, you can hit tab to autocomplete and shift-tab to see documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\n",
    "                   'Eric Idle', 'Terry Jones', 'Michael Palin'])\n",
    "monte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Third\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Recipe Database\n",
    "\n",
    "* Let's walk through the recipe database example from the Python Data Science Handbook\n",
    "* There are a few concepts and commands I haven't yet covered, but I'll explain them as I go along\n",
    "* Download the recipe file from [this link](https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz) or run the cell below if you are on JupyterHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipes = pd.read_json(\"https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz\", \n",
    "                       compression='gzip',\n",
    "                       lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have downloaded the data and loaded it into a dataframe directly from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are nearly 200,000 recipes, and 17 columns.\n",
    "Let's take a look at one row to see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the first item in the DataFrame\n",
    "recipes.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the first five items in the DataFrame\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information there, but much of it is in a very messy form, as is typical of data scraped from the Web.\n",
    "In particular, the ingredient list is in string format; we're going to have to carefully extract the information we're interested in.\n",
    "Let's start by taking a closer look at the ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the length of the ingredients string\n",
    "recipes['ingredients'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# which row has the longest ingredients string\n",
    "recipes['ingredients'].str.len().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use iloc to fetch that specific row from the dataframe\n",
    "recipes.iloc[135598]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the ingredients string\n",
    "recipes.iloc[135598]['ingredients']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* WOW! That is a lot of ingredients! That might need to be cleaned by hand instead of a machine\n",
    "* What other questions can we ask of the recipe data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many breakfasts?\n",
    "recipes.description.str.contains('[Bb]reakfast').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many have cinnamon as an ingredient?\n",
    "recipes.ingredients.str.contains('[Cc]innamon').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many misspell cinnamon as cinamon?\n",
    "recipes.ingredients.str.contains('[Cc]inamon').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets\n",
    "\n",
    "One of the tasks you will need to do for your final project, and in the wide world of data munging, is combining disparate datasets together into a single set. \n",
    "\n",
    "### Merging the same Data\n",
    "\n",
    "Sometimes you have the same data, but it has been broken up over multiple files (over time or some other distinction). Ultimately what you want is a single dataframe that contains all the data from separate files (or dataframes). Let's load some data into three separate dataframes and then smoosh them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data for April, May, and June\n",
    "april_url = \"https://data.wprdc.org/datastore/dump/043af2a6-b58f-4a2e-ba5f-7ef868d3296b\"\n",
    "may_url = \"https://data.wprdc.org/datastore/dump/487813ec-d7bc-4ff4-aa74-0334eb909142\"\n",
    "june_url = \"https://data.wprdc.org/datastore/dump/d7fd722c-9980-4f7a-a7b1-d1a55a365697\"\n",
    "\n",
    "april_acj_data = pd.read_csv(april_url)\n",
    "may_acj_data = pd.read_csv(may_url)\n",
    "june_acj_data = pd.read_csv(june_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inspect the dataframes\n",
    "april_acj_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inspect the dataframes\n",
    "may_acj_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inspect the dataframes\n",
    "june_acj_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have three dataframes with the Allegheny County Jail census for three months. All of the columns are the same so the merge will be relatively straightforward, we just have to concatinate the three dataframes together. Following the Pandas documentation on [Merging, joining, and concatinating object](https://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects), I will use the [`concat()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html#pandas.concat) function to smoosh the three dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the dataframes I want to smoosh together into a python list\n",
    "monthly_dataframes = [april_acj_data, may_acj_data, june_acj_data]\n",
    "\n",
    "# use the concat fuction to put them together into a new dataframe\n",
    "ajc_data = pd.concat(monthly_dataframes)\n",
    "\n",
    "# sample 5 random rows from the dataframe so I can (hopefully) see entries\n",
    "# from each of the three months\n",
    "ajc_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `concat()` function to merge identical datasets together. But what if your data don't line up? What do you do then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging different data with overlapping columns\n",
    "\n",
    "The [PGH 311 Data](https://data.wprdc.org/dataset/311-data) provides a good example for connecting datasets that don't line up, but are still connectable because they share columns. First, let's load up the 311 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 311 data into a dataframe\n",
    "url = \"https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4\"\n",
    "pgh_311_data = pd.read_csv(url)\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one of the things I like to do with the 311 data is count requests by type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all the unique values in the column REQUEST_TYPE\n",
    "pgh_311_data['REQUEST_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a HUGE horizontal bar chart so we can see the distribution of 311 complaints\n",
    "# it took me a bunch of guesses to figure out the right figure size\n",
    "pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a HUGE horizontal bar chart so we can see the distribution of 311 complaints\n",
    "# it took me a bunch of guesses to figure out the right figure size\n",
    "pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh(figsize=(10,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! But there are 284 different types of requests, this is not very useful. Fortunately the 311 data has a [code book](https://data.wprdc.org/dataset/311-data/resource/7794b313-33be-4a8b-bf80-41751a59b84a) that rolls the request types into a set of higher level categories. Note, the code book is a Microsoft Excel file so we got to use the `read_excel()` function instead of `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 data code book\n",
    "url = \"https://data.wprdc.org/dataset/a8f7a1c2-7d4d-4daa-bc30-b866855f0419/resource/7794b313-33be-4a8b-bf80-41751a59b84a/download/311-codebook-request-types.xlsx\"\n",
    "pgh_311_codes = pd.read_excel(url) # parse the excel sheet\n",
    "pgh_311_codes.sample(10) # pull ten random rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we loaded the codebook into a separate dataframe and if we look at it we can see how the `REQUEST_TYPE` from the data corresponds to `Issues` in the code book. Additionally, we can see how there is a higher level `Catagory` associated with each issue in the code book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row for \"Potholes\"\n",
    "query = pgh_311_codes['Issue'] == 'Potholes'\n",
    "pgh_311_codes[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row for \"Weeds/Debris\"\n",
    "query = pgh_311_codes['Issue'] == 'Weeds/Debris'\n",
    "pgh_311_codes[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row for \"Building Maintenance\n",
    "query = pgh_311_codes['Issue'] == 'Building Maintenance'\n",
    "pgh_311_codes[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the data you will notice that both \"Weeds/Debris\" and \"Building Maintenance\" belong to the same category of \"Neighborhood Issues.\" Using this mapping we can hopefully make a bit more sense of the data.\n",
    "\n",
    "Now what we need to do is `merge()` the data. We can look to the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) to provide some explaination about how use use this fuction to combine two datasets with overlapping columns with different names.\n",
    "\n",
    "In our case what we want to do is *merge* the codebook into the 311 data and add a new column for the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes on the REQUEST_TYPE and ISSUE columns\n",
    "pgh_311_data_merged = pgh_311_data.merge(pgh_311_codes, left_on=\"REQUEST_TYPE\", right_on=\"Issue\")\n",
    "pgh_311_data_merged.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the numbers of unique values in the Category column\n",
    "pgh_311_data_merged['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much more managably set of categorical values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bar chart of the categories for the merged data\n",
    "pgh_311_data_merged['Category'].value_counts(ascending=True).plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can dive into specific categories and find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query mask for rows where the Category is equal to the value \"Road/Street Issues\"\n",
    "query = pgh_311_data_merged['Category'] == \"Road/Street Issues\"\n",
    "\n",
    "# find the rows matching the query, select the Issue column and count the unique values \n",
    "pgh_311_data_merged[query]['Issue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query mask for rows where the Category is equal to the value \"Road/Street Issues\"\n",
    "query = pgh_311_data_merged['Category'] == \"Road/Street Issues\"\n",
    "\n",
    "# find the rows matching the query, select the Issue column and count the unique values and make a bar chart\n",
    "pgh_311_data_merged[query]['Issue'].value_counts(ascending=True).plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An isolated groupby from the Counting categorical values example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* count the attendance per center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing with pands\n",
    "center_attendance_pandas.groupby('center_name')['attendance_count'].sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting Data\n",
    "\n",
    "Let's look at one of the most exciting datasets in the WPRDC, the [Daily Community Center Attendance records](https://data.wprdc.org/dataset/daily-community-center-attendance)! WOWOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://data.wprdc.org/datastore/dump/b7cb30c8-b179-43ff-8655-f24880b0f578\"\n",
    "\n",
    "# load data and read in the date column as the row index\n",
    "data = pd.read_csv(data_url, index_col=\"date\", parse_dates=True)\n",
    "data = data.drop(columns=\"_id\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like?\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pivot the data so the center names are columns and each row is the number of people attending that community center per day. This is basically rotating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pivot function to make column values into columns\n",
    "data.pivot(columns=\"center_name\", values=\"attendance_count\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of NaN, and not the tasty garlicy kind either.\n",
    "\n",
    "We might want to break this apart for each Community Center. We can start by inspecting the number rows per center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows per center\n",
    "data.groupby(\"center_name\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of community centers that don't have a lot of numbers because either 1) they are not very popular or 2) they don't report their daily attendance (more likely given how man NaNs we saw above).\n",
    "\n",
    "What we will do is create a custom filter function that we will apply to ever row in the dataframe using the [groupby filter function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.filter.html). This is some knarly stuff we are doing here. This isn't the plain old filter function, this is a special filter fuction (part of the groupby functionality) that requires you to create a special function to apply to each row. In our case we will make a little function that takes a value and tests to see if it is create than a threshold value (in our case 1000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function we will use to perform a filtering \n",
    "# operation on the data\n",
    "# filter out centers that have less then 1000 total entries\n",
    "def filter_less_than(x, threshold):\n",
    "    #print(x)\n",
    "    if len(x) > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "# def filter_less_than(x):\n",
    "#     if len(x) > 1000:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# use the custom function to filter out rows\n",
    "popular_centers = data.groupby(\"center_name\").filter(filter_less_than, \n",
    "                                                     threshold=1000)\n",
    "# look at what centers are in the data now\n",
    "popular_centers.groupby(\"center_name\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the popular community centers\n",
    "popular_centers.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pivot function to make rows into columns with only the popular community centers\n",
    "pivoted_data = popular_centers.pivot(columns=\"center_name\", values=\"attendance_count\")\n",
    "pivoted_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still NaN-y, but not as bad. Now lets see what these data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "pivoted_data.plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the [cumulative sum](http://www.variation.com/cpa/help/hs108.htm) to see if the attendance is above or below average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cumulative sum for every column and make a chart\n",
    "pivoted_data.cumsum().plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Brookline is the winner here, but attendance has tapered off in the past couple years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and compute the monthly totals for the popular community centers\n",
    "pivoted_data.resample(\"M\").sum().plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like monthly is too messy, maybe by year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly resample to monthly, compute the totals, and plot\n",
    "pivoted_data.resample(\"Y\").sum().plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot(columns=\"center_name\", values=\"attendance_count\").resample(\"Y\").sum().plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the total number of attendance per year per popular community center gives us a bit more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split, Apply, Combine with numeric data\n",
    "\n",
    "* The 311 complaints are mainly categorical data, which doesn't let use do more mathematical aggregations\n",
    "* Lets grab a different dataset from the WPRDC, the [Allegheny County Jail Daily Census](https://data.wprdc.org/dataset/allegheny-county-jail-daily-census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab three months of data\n",
    "january17_jail_census = pd.read_csv(\"https://data.wprdc.org/datastore/dump/3b5d9c45-b5f4-4e05-9cf1-127642ad1d17\",\n",
    "                                  parse_dates=True,\n",
    "                                  index_col='Date')\n",
    "feburary17_jail_census = pd.read_csv(\"https://data.wprdc.org/datastore/dump/cb8dc876-6285-43a8-9db3-90b84eedb46f\",\n",
    "                                   parse_dates=True,\n",
    "                                  index_col='Date')\n",
    "march17_jail_census = pd.read_csv(\"https://data.wprdc.org/datastore/dump/68645668-3f89-4831-b1de-de1e77e52dd3\",\n",
    "                                parse_dates=True,\n",
    "                                index_col='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "january17_jail_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the concat function to combine all three into one dataframe\n",
    "# Remember I need to make a list of the all the dataframes for\n",
    "# the concat fuction\n",
    "jail_census = pd.concat([january17_jail_census, \n",
    "                         feburary17_jail_census, \n",
    "                         march17_jail_census])\n",
    "jail_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the \"_id\" column because it is not useful\n",
    "jail_census.drop(\"_id\", axis=1, inplace=True)\n",
    "jail_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get just the first day in Feburary 2017\n",
    "jail_census.loc[\"2017-02-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the average age ate booking by gender for Febuary 1st, 2017\n",
    "jail_census.loc['2017-02-01'].groupby('Gender')['Age at Booking'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the average age at booking by race for Febuary 1st, 2017\n",
    "jail_census.loc['2017-02-01'].groupby('Race')['Age at Booking'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the [data dictionary](https://data.wprdc.org/dataset/allegheny-county-jail-daily-census/resource/f0550174-16b0-4f6e-88dc-fa917e74b56c) we can see the following mapping for race categories\n",
    "```\n",
    "Race of Inmate\n",
    "A-ASIAN OR PACIFIC ISLANDER\n",
    "B-BLACK OR AFRICAN AMERICAN\n",
    "H-HISPANIC \n",
    "I-AMERICAN INDIAN OR ALASKAN NATIVE\n",
    "U-UNKNOWN\n",
    "W-WHITE\n",
    "```\n",
    "The `x` category hasn't been described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many total rows in the dataset have \"x\" for race\n",
    "jail_census['Race'].value_counts()['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the statistical summary of age at booking by gender for Febuary 1st, 2017\n",
    "jail_census.loc['2017-02-01'].groupby('Gender')['Age at Booking'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the difference between Age at Booking and current age\n",
    "age_difference = jail_census.loc['2017-02-01']['Current Age'] - jail_census.loc['2017-02-01']['Age at Booking']\n",
    "age_difference.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the average age for each day\n",
    "jail_census.resample(\"D\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is with that NaNs?\n",
    "jail_census.loc['2017-03-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the number of inmates\n",
    "jail_census.resample(\"D\").size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Time\n",
    "\n",
    "Often there is date/time data in one of the columns of your dataset. In this case `CREATED_ON` appears to be a date/time for when the 311 complaint was lodged. Unless you specify `parse_dates=True` when you call the read_csv, you will need to re-parse your date/time column into the correct datatype. \n",
    "\n",
    "\n",
    "For example, if we look at the datatypes for all of the columns in our potholes in Bloomfield dataset we can see been parsed as dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the datatypes for each column in the data\n",
    "bloomfield_pothole_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix that! First we parse the `CREATED_ON` column using the `to_datetime()` function. What this does is loop over every value in the column and convert it to a datetime data type. \n",
    "\n",
    "\n",
    "**Important Note**: Even though we just want to look the potholes for Bloomfield, we need to do this operation on the full data, not the subselection of potholes in Bloomfield. Why? It has to do with the way Pandas manages the data behind the scenes, our `bloomfield_pothole_data` is actually a *view* into the larger dataframe, `pgh_311_data_merged`. This means we should change the original data because then we'll see it reflected in our bloomfield/potholes subset. Changing the originaly data as opposed to our subset is also good practice because we might want to look at the temporal distribution for other types of 311 request or other neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the CREATED_ON column with parsed dates\n",
    "pgh_311_data_merged['CREATED_ON'] = pd.to_datetime(pgh_311_data_merged['CREATED_ON'])\n",
    "pgh_311_data_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, now that Pandas is aware of dates we can start doing operations on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReCreate a query mask for potholes\n",
    "query_potholes = pgh_311_data_merged['REQUEST_TYPE'] == \"Potholes\"\n",
    "# Create a query mask for bloomfield\n",
    "query_bloomfield = pgh_311_data_merged['NEIGHBORHOOD'] == \"Bloomfield\"\n",
    "\n",
    "# create a new dataframe that queries potholes AND bloomfield\n",
    "bloomfield_pothole_data = pgh_311_data_merged[query_potholes & query_bloomfield]\n",
    "# inspect the new dataframe\n",
    "print(bloomfield_pothole_data.shape)\n",
    "bloomfield_pothole_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the datatype has changed in our subset of the data\n",
    "bloomfield_pothole_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a temporal index by setting it equal to CREATED_ON\n",
    "bloomfield_pothole_data.index = bloomfield_pothole_data['CREATED_ON']\n",
    "bloomfield_pothole_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample (grouping) by month (\"M\") and counting the number of complaints\n",
    "bloomfield_pothole_data['REQUEST_ID'].resample(\"M\").count().plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Bloomfield had a MASSIVE spike in pothole complaints this past winter. You can see there is a pattern, it is lowest right before the new year and then it springs in the spring and then it falls again in the fall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data\n",
    "\n",
    "* Bringing disparate datasets together is one of the more powerful features of Pandas\n",
    "* Like with Python lists, you can `append()` and `concat()` Pandas `Series` and `Dataframes`\n",
    "* These functions work best for simple cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatinate two series together\n",
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatinate two dataframes\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatinate dataframes horizontally\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"C\":[\"C1\", \"C2\"],\n",
    "                    \"D\":[\"D1\",\"D2\"]},index=[1,2])\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What happens when indexes don't line up\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a hierarchical index\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2], keys=[\"df1\", 'df2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Joining\n",
    "\n",
    "* While `concat()` is useful it lacks the power to do complex data merging\n",
    "* For example, I have two tables of different data but one overlapping column\n",
    "* This is where the `merge()` function becomes useful because it lets you *join* datasets\n",
    "* The concept of \"join\" has lots of theory and is a richly developed method for *joining* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-to-one joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two dataframes with one shared column\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display df1\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display df2\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge df1 and df2 intzo a new dataframe df3\n",
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The new dataframe `df3` now has all of the data from df1 and df2\n",
    "* The `merge` function automatically connected the two tables on the \"employees\" column\n",
    "* But what happens when your data don't line up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-one joins\n",
    "\n",
    "* Sometimes there isn't a one to one relationshp between rows in the two datasets\n",
    "* A *many-to-one* join lets you combine these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make another dataframe about the supervisor for each group\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge df3 from above with the supervisor info in df4\n",
    "pd.merge(df3,df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice how the information about Guido, the manager for Engineering, is repeated.\n",
    "* While this might seem like duplicated data, it makes it easier to quickly look up Jake and Lisa's supervisor without consulting multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-many joins\n",
    "\n",
    "* Let's combine the employee information with skills information\n",
    "* Notice there isn't a one to one or even a one to many relationship between these tables\n",
    "* Each group can have multiple skills, so **what do you think will happen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the employee table specified above\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new dataframe with skills information\n",
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR', 'Librarian'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization', 'nunchucks']})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amazing, Pandas merge capabilities are very useful\n",
    "* But what do you do if the names of your columns don't match?\n",
    "* You could change column names...\n",
    "* But that is crazy! Just use the `left_on` and `right_on` parameters to the `merge()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the employee table specified above\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new salary table, but use \"name\" instead of \"employee\" for the column index\n",
    "df3 = df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets try and merge them without specifying what to merge on\n",
    "pd.merge(df1, df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the column names I should specify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets specify the column name \n",
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice we now have a redundant employee/name column, this is a by-product of merging different columns\n",
    "* If you want to get rid of it you can use the `drop` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the name column, axis=1 means axis='col', which is confusing\n",
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\" ).drop(\"name\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Pandas\n",
    "\n",
    "* You can plot directly from `pandas` data structures\n",
    "* Pandas [has its own interface](https://pandas.pydata.org/pandas-docs/stable/visualization.html#) to matplotlib tied directly to the `Series` and `Dataframe` data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to import numpy for generating random data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Important!** You need the following code to render plots inside of Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tell matplotlib to render visualizations in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create some random data\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# put that data into a dataframe\n",
    "df = pd.DataFrame({\"y\":np.sin(x), \"z\":np.cos(x)}, index=x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data using the plot method\n",
    "df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Basically, you can add a `.plot()` to the end of any Pandas datastructure and it will make a best guess as to the best way to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data in a Series with the plot method\n",
    "pd.Series(np.random.randint(0,10,10)).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, be careful calling `.plot()` all willy nilly since it doesn't always produce sensible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some random time series data and create a default plot\n",
    "random_series = pd.Series(np.random.randn(1000), \n",
    "                          index=pd.date_range('1/1/2000', periods=1000))\n",
    "random_series.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is cool is you can often use the `.plot()` method after performing some computation on the data\n",
    "* For example, we can calculate the [cumulative sum](http://www.variation.com/cpa/help/hs108.htm) (the cumulative sum of differences between the values and the average)\n",
    "    * Sloping up means above average, sloping down means below average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative sum of a Series\n",
    "random_series.cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `.plot()` trick also works with Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with four columns and create a default plot\n",
    "df = pd.DataFrame(np.random.randn(1000, 4), index=random_series.index, columns=list('ABCD'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just plot the dataframe and see what happens\n",
    "df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Messy! Let's try the cumulative sum trick and see if that looks any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative sum of each column\n",
    "df.cumsum().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With pandas you can specify the kind of visualization with the `kind` parameter to `plot()`\n",
    "* The default isn't always what you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of the columns\n",
    "df.sum().plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a *bad* visualization, the line imply an ordered relationship between the four categories\n",
    "* Let's use a bar chart instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of the columns as bars\n",
    "df.sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Almost got it, but the labels on the x axis are a bit wonky. \n",
    "* Let's look at the documentation and see if we can find a fix\n",
    "\n",
    "#### Quick Exercise\n",
    "\n",
    "* Find the documentation for the `plot()` method of a Pandas `Series`\n",
    "    * *HINT*: Try Googling\n",
    "* What parameter will fix the x labels so they are easier to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = pd.Series([1,5,2,5], index=[\"cats\", \"dogs\", \"chickens\", \"spiders\"])\n",
    "\n",
    "animals.plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to the exercise\n",
    "animals = pd.Series([1,5,2,5], index=[\"cats\", \"dogs\", \"chickens\", \"spiders\"])\n",
    "\n",
    "animals.plot(kind=\"bar\", rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Plot types\n",
    "\n",
    "* Pandas provides a quick and easy interface to a bunch of different plot types\n",
    "* You don't even have to load `matplotlib` (although you do need `%matplotlib inline`)\n",
    "* The secret to plotting is Googling, looking at other people's code, and trying things until it works\n",
    "    * At least, that is how I do it\n",
    "\n",
    "\n",
    "* What is nice about pandas/matplotlib integration is pandas will handle a lot of the boilerplate code for you\n",
    "* Then you pass parameters to the `plot()` method to determine how the graph should look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some random categorical data\n",
    "df2 = pd.DataFrame(np.random.randint(1,100,size=(7,4)), \n",
    "                   columns=['Carbs', 'Fats', 'Proteins', 'Other'], \n",
    "                   index=[\"M\",\"Tu\",\"W\",\"Th\",\"F\",\"Sa\",\"Su\",])\n",
    "\n",
    "# Plot a bar chart\n",
    "df2.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bar charts can also be called directly using the `bar()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a bunch of parameters for these methdos that let you tweak the vizualization\n",
    "* For example, the `stacked` parameter stacks the categorical values so you can easily compare within and across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot.bar(stacked=True, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Try experimenting with the other plot types\n",
    "* Do they make sense for these data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move the cursor to the right of the period and hit tab\n",
    "df2.plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try another plot type\n",
    "# move the cursor to the right of the period and hit tab\n",
    "df2.plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Time\n",
    "\n",
    "* One of the most powerful features of Pandas is its time series functionality\n",
    "* Dates and time are a Python and Pandas data type (like integers and strings)\n",
    "* By using the `datetime` data types you can do advanced, time-centric analysis\n",
    "* One thing to remember about computers is they are *very* specific\n",
    "    * *Time stamps* - a specific moment in time (July 4th, 2017 at 7:52am and 34 seconds)\n",
    "    * *Time intervals* - a length of time with start and end points (The year 2017)\n",
    "    * *Time duration* - a specific length of time (a year, a month, a day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datetime in pure Python\n",
    "import datetime\n",
    "\n",
    "date = datetime.datetime(year=2017, month=6, day=13)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what is that date's month?\n",
    "date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what is that date's day?\n",
    "date.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the parser function in the datautil library to parse human dates\n",
    "from dateutil import parser\n",
    "date = parser.parse(\"4th of July, 2017\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the month\n",
    "date.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Try some different date strings, see how smart Python can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_date = parser.parse(\"<your date string here\")\n",
    "my_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use [*string format codes*](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior) for printing dates and time in different formats (especially useful for making human readable dates)\n",
    "* Pass a format string to the `strftime()` method to print out a pretty date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the weekday \n",
    "date.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date.strftime(\"%B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Try some of the different string format codes and see what happens\n",
    "date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Try combining a few of them together with punctuation too\n",
    "date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with time in Pandas\n",
    "\n",
    "* Just like how Pandas has its own datatypes for numbers, Pandas has its own dates and times (to support more granularity)\n",
    "* If you have a lot of dates, it is often useful to use the Pandas functions over the native Python functions\n",
    "* Pandas is most powerful when you index by time using the `DatetimeIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Series with a DateTime index\n",
    "index = pd.DatetimeIndex(['2014-03-04', '2014-08-04',\n",
    "                          '2015-04-04', '2015-09-04',\n",
    "                          '2016-01-01', '2016-02-16'])\n",
    "data = pd.Series([0, 1, 2, 3, 4, 5], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that the index is made of DateTimes we can index using date strings\n",
    "* Note, this only works on strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the value for a specific day\n",
    "data[\"2015-04-04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab a slice between two dates\n",
    "data['2014-08-01':'2016-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give me everything from 2015\n",
    "data['2015']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas has some functions to make parsing dates easy too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the to_datetime function instead of the parser function\n",
    "date = pd.to_datetime(\"4th of July, 2017\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use string format codes to get the weekday\n",
    "date.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me today's date\n",
    "today = pd.to_datetime(\"today\")\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* That is the day, but also the exact time... \n",
    "* Timestamps must always be a specific moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "* Use the [*string format codes*](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior) to print today's date in the \"YYYY-MM-DD\" format. HINT: You will have to combine multiple codes and dashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the ??? with the write string format code\n",
    "print(today.strftime(\"???\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with time on real data\n",
    "\n",
    "* Let's look at the [311 data for the city of Pittsburgh](https://data.wprdc.org/dataset/311-data) from the WPRDC\n",
    "* Did you know, you can give the URL directly to Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 data directly from the WPRDC\n",
    "pgh_311_data = pd.read_csv(\"311_data.csv\")\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok, now we have the data, but we need it to be indexed by date\n",
    "* **What column has the date information?**\n",
    "* **What format do you think that column is currently in?**\n",
    "* **What function might we use to convert that column into dates?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgh_311_data['CREATED_ON'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the \"CREATED_ON\" column to dates\n",
    "pd.to_datetime(pgh_311_data['CREATED_ON']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can convert the \"CREATED_ON\" column to Pandas `datetime` objects\n",
    "* Now we have to set that to the dataframe's index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index of pgh_311_data to be the parsed dates in the \"CREATED_ON\" column\n",
    "pgh_311_data.index = pd.to_datetime(pgh_311_data['CREATED_ON'])\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do'h, now we have CREATED_ON twice, that isn't very tidy\n",
    "* We can also skip this extra conversion step entirely by specifying the index column and date parsing in `read_csv()` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the 311 data directly from the WPRDC and parse dates directly\n",
    "pgh_311_data = pd.read_csv(\"311_data.csv\",\n",
    "                           index_col=\"CREATED_ON\", \n",
    "                           parse_dates=True)\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgh_311_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that the dataframe has been indexed by time we can select 311 complains by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select 311 complaints on January 1st, 2016\n",
    "pgh_311_data['2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the times just around the new years celebration\n",
    "pgh_311_data[\"2015-12-31 20:00:00\":\"2016-01-01 02:00:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Someone clearly had a very roudy new years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Using the timeseries index selection, select the complaints made today \n",
    "* Bonus, try and write your code so it will work on any day you execute it\n",
    "    * *hint*: try the `pd.datetime('today')` \n",
    "    * *Another hint*: Remember the DateTime gives you the exact time \n",
    "    * *Yet another hint*: Datetime indexing only works with string representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pgh_311_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a Pandas datetime for today\n",
    "today = pd.to_datetime(\"today\")\n",
    "formatted_today_string = today.strftime(\"%Y-%m-%d\")\n",
    "print(today)\n",
    "print(formatted_today_string)\n",
    "\n",
    "# use Pandas date string indexing to retrieve all rows for this today's date\n",
    "todays_311s = pgh_311_data[formatted_today_string]\n",
    "todays_311s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping time with the resample method\n",
    "\n",
    "* Instead of using the `groupby()` method, you use the `resample()` method to *split* time into groups\n",
    "* Then you can *apply* the regular aggregation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the mean of complaints per quarter...note this doesn't make sense, but works anyway\n",
    "pgh_311_data.resample(\"Q\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the number of complaints per month\n",
    "pgh_311_data.resample(\"M\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok, these data are *begging* to be visualized, so I'm going to give you a teaser of next week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load up the data visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a graph of the monthly complaint counts\n",
    "pgh_311_data['REQUEST_ID'].resample(\"M\").count().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the code above, but re-sampling based upon different date periods. The strings for specifying an offset are located [here](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a different resampling here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try yet another resampling here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OK, we've done some \"fun\" stuff with Time, but maybe we want to start doing deeper analysis\n",
    "* To do that, we need to know what all these columns mean?\n",
    "* Fortunately, this dataset has a [data dictionary](https://data.wprdc.org/dataset/311-data/resource/d3e98904-4a86-45fb-9041-0826ab8d56d0), which provides a bit more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Data\n",
    "\n",
    "* It is sometimes helpful to think of a Pandas Dataframe as a little database. \n",
    "* There is data and information stored in the Pandas Dataframe (or Series) and you want to *retrieve* it.\n",
    "* Pandas has multiple mechanisms for getting specific bits of data and information from its data structures. The most common is to use *masking* to select just the rows you want. \n",
    "* Masking is a two stage process, first you create a sequence of boolean values based upon a conditional expression--which you can think of as a \"query\"--and then you index your dataframe using that boolean sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_attendance_pandas.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if we only wanted to see attendance for Brookline Community Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = center_attendance_pandas[\"center_name\"] == \"Brookline Community Center\"\n",
    "\n",
    "brookline_center_attendance = center_attendance_pandas[query]\n",
    "\n",
    "brookline_center_attendance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create queries for brookline and greenfield\n",
    "brookline_query = center_attendance_pandas[\"center_name\"] == \"Brookline Community Center\"\n",
    "greenfield_query = center_attendance_pandas[\"center_name\"] == \"Magee Community Center\"\n",
    "\n",
    "# use the boolean OR operator to select both community centers\n",
    "center_attendance_pandas[brookline_query | greenfield_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the 311 Data\n",
    "\n",
    "* Now we can use what we have learned to do some exploratory data analysis on the 311 data\n",
    "* First, lets use the `sample()` method to grab 10 random rows so we can get a feel for the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample 10 random rows from the dataframe\n",
    "pgh_311_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "\n",
    "* What are the possible *origins* of complaints?\n",
    "* How many complaints are coming from each source?\n",
    "\n",
    "*HINT*: Scroll back up to the top of to look at the Dataframes refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgh_311_data['REQUEST_ORIGIN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* *Group* the complaints *by* neighborhood and get the *size* of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgh_311_data.groupby('NEIGHBORHOOD').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note, for just counting the groupby and value_counts are equivalent\n",
    "# There is more than one way to skin the cat (or panda)\n",
    "pgh_311_data['NEIGHBORHOOD'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* *Group* the complaints *by* type and get the *count* for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgh_311_data.groupby(\"REQUEST_TYPE\")['REQUEST_TYPE'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This categorical data is far too granular. \n",
    "Fortunately, if we look at the [311 Data](https://data.wprdc.org/dataset/311-data) we can see there is a [311 Issue and Category Codebook](https://data.wprdc.org/dataset/311-data/resource/40ddfbed-f225-4320-b4d2-7f1e09da72a4). Click on that link and check out the Google Sheets preview of that data.\n",
    "\n",
    "https://data.wprdc.org/dataset/311-data/resource/40ddfbed-f225-4320-b4d2-7f1e09da72a4\n",
    "\n",
    "What we need to do is download the CSV from Google Sheets directly into a Pandas dataframe, but this is actually a bit tricky because Google won't easily give us a link to the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I googled \"pandas dataframe from google sheets\"\n",
    "# and found a solution on Stackoverflow\n",
    "# https://stackoverflow.com/a/35246041\n",
    "issue_category_mapping = pd.read_csv('https://docs.google.com/spreadsheets/d/' + \n",
    "                   '1DTDBhwXj1xQG1GCBKPqivlzHQaLh2HLd0SjN1XBPUw0' +\n",
    "                   '/export?gid=0&format=csv')\n",
    "issue_category_mapping.head(5)  # Same result as @TomAugspurger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Merge the `pgh_311_data` with the `issue_category_mapping` so we can count the number of complaints per category\n",
    "* *HINT*: You will need to specify the `left_on` and `right_on` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new merged dataframe\n",
    "merged_311_data = pd.merge(pgh_311_data, \n",
    "         issue_category_mapping,\n",
    "         left_on=\"REQUEST_TYPE\",\n",
    "         right_on=\"Issue\")\n",
    "\n",
    "merged_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of redundant columns\n",
    "merged_311_data.drop(['Definition','Department', 'Issue'], \n",
    "                     axis=1, \n",
    "                     inplace=True)\n",
    "merged_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Now that we have category data, count the number of complaints by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_311_data.groupby(\"Category\")['Category'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_311_data.groupby(\"Category\").size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecting data in a Dateframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select only rows where NEIGHBORHOOD equals \"Greenfield\" and then count how many complaints came from each source\n",
    "merged_311_data[merged_311_data['NEIGHBORHOOD'] == 'Greenfield'].groupby('REQUEST_ORIGIN').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge\n",
    "\n",
    "Querying and subsetting - Masking. Variables\n",
    "\n",
    "EXERCISE - time indexing and subsetting data and visuzlaing that subset\n",
    "\n",
    "Create a sub dataset of just the potholes in a neighborhood\n",
    "\n",
    "Create a plot of number of pothole complaints per month of that data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a sub dataset of just the potholes in Highland Park \n",
    "potholes = data_311['REQUEST_TYPE'] == \"Potholes\"\n",
    "\n",
    "highland_park= data_311['NEIGHBORHOOD'] == \"Highland Park\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "highlandpark_potholes = data_311[potholes & highland_park]\n",
    "\n",
    "print(highlandpark_potholes.shape)\n",
    "highlandpark_potholes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot the number of pothole compaints per month of that data set\n",
    "\n",
    "\n",
    "#Change created on to a date-time object \n",
    "#WHERE SHOULD WE PUT THIS????\n",
    "data_311['CREATED_ON'] = pd.to_datetime(data_311['CREATED_ON'])\n",
    "\n",
    "#Change the index to a date time object \n",
    "highlandpark_potholes.index = highlandpark_potholes['CREATED_ON']\n",
    "\n",
    "\n",
    "#Plot the figure \n",
    "highlandpark_potholes['REQUEST_ID'].resample(\"M\").count().plot(title=\"Highland Park Potholes\", figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
